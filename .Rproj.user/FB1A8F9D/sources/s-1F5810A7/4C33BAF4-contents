---
title: "Accessing WRDS Remotely with R"
author: "David Sovich"
date: "February 11, 2019"
output:
  html_document:
    self_contained: no
    number_sections: true
    highlight: pygments
    toc: true
    toc_float: true
    toc_depth: 3
    theme: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

WRDS provides two remote interfaces for R users. 

The first interface allows R users to access WRDS data from their local RStudio Session. This is optimal for users who work with smaller datasets (e.g., Compustat) and users who constantly need to retrieve new data. 

The second interface allows R users to deploy R on a node of WRDS's computing cluster, known as the WRDS cloud. This is optimal for users who work with larger datasets (e.g., TAQ) and require more intense computing power. 

The below sections outline how to interact with WRDS from RStudio or on the cloud. 

A more detailed guide from WRDS is available at: <https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/>. 

# Accessing WRDS from your local RStudio Session

You can connect to the WRDS family of databases from your local instance of RStudio. 

In general, connecting to WRDS with R is just like connecting to any other remote database. However, there are a few items to note: 

1. You cannot use create temporary tables or views in the WRDS database from RStudio. This will give "lazy" packages, such as dplyr, an advantage over SQL-based packages that require you to either nest queries or bring the data down all at once. 

1. WRDS is currently structured as an ssl encrypted PostgreSQL database. PostgreSQL allows for window and other analytic functions in SQL (similar to Cloudera Impala or Hive). For a primer on analytic functions in PostgreSQL, please see the following link: <http://www.postgresqltutorial.com/postgresql-window-function/>. 

1. WRDS was previously structured as a set of flat SAS files. R users had to install Java and JDBC drivers for connectivity, and they were unable to access memory intensive datasets such as TAQ. The transition to PostgreSQL has made most of the previous packages obsolete and incompatible (e.g., see my previous work at <http://students.olin.wustl.edu/~sovichd/rWRDS.txt>). 

## Connecting to the WRDS database

Begin by loading the following packages:

```{r packages, warning=FALSE, results = "hide", message=FALSE}
library(tidyverse)
library(RPostgres)
library(DBI)
```

Now create a connection to the WRDS PostgreSQL database using the following code:

```{r connection}

wrds = dbConnect(Postgres(),
                  host='wrds-pgdata.wharton.upenn.edu',
                  port=9737,
                  dbname='wrds',
                  user = Sys.getenv("WRDS_NAME"), # input your login here; mine is stored in the .Renviron
                  password = Sys.getenv("WRDS_PASS"), # input your password here 
                  sslmode='require')

```

You are now connected to the WRDS database from RStudio!

## Querying the WRDS database

We can query the WRDS database using the DBI package or dplyr package (along with many others). 

The DBI package offers a more traditional SQL interface (e.g., dbGetQuery). The dplyr package abstracts from the SQL interface in favor of tidyverse grammar. 

### Available data in WRDS

To see the list of available schemas (data vendors) and tables in the database, we need to query the metadata tables. 

I recommend using DBI::dbGetQuery to accomplish this tasks. I find that dplyr is much slower dealing with schemas in the WRDS environment than DBI. 

You can extract the set of available schemas from WRDS using the following code:

```{r schemas}
db_schema = dbGetQuery(wrds, paste("SELECT DISTINCT table_schema",
                                    "FROM information_schema.tables",
                                    "WHERE table_type = 'VIEW' OR table_type = 'FOREIGN TABLE'",
                                    "ORDER BY table_schema"))
head(db_schema, 5)
```

For a given schema, such as annual Compustat, you can extract the set of available tables and the variables in a specific table as follows:

```{r tables}

#Extract set of tables in annual Compustat schema
comp_tables = dbGetQuery(wrds, paste("SELECT DISTINCT table_name",
                                   "FROM information_schema.columns",
                                   "WHERE table_schema = 'compa'",
                                   "ORDER BY table_name"))
head(comp_tables, 5)

#Extract variables in funda table
comp_variables = dbGetQuery(wrds, paste("SELECT column_name, udt_name",
                                        "FROM information_schema.columns",
                                        "WHERE table_schema = 'compa' AND table_name = 'funda'",
                                        "ORDER BY column_name"))
head(comp_variables, 5)

```

Alternatively, you can view the list of schemas, tables, and variables in WRDS at the following link: <https://wrds-web.wharton.upenn.edu/wrds/tools/variable.cfm>. 

### Querying the database with DBI

DBI offers a traditional SQL interface for working with WRDS. Unforunately though, you cannot create temporary tables or views in the WRDS environment. This means you must perform all operations in one SQL query (via nested queries) before pulling down the data.

You can use either the `dbSendQuery` or `dbGetQuery` functions to retrieve the data. However, since you cannot create temporary tables or views, there is no real reason for using `dbSendQuery` followed by `fetch`. 

An example is given below:

```{r createtable}

#Download data from year 2000 using dbGetQuery (limit to 100 rows)
comp_data = dbGetQuery(wrds, paste("SELECT gvkey, fyear, at, revt, ib",
                                   "FROM compa.funda",
                                   "WHERE (fyear = 2000) AND (consol = 'C') AND (datafmt = 'STD') AND (indfmt IN('INDL','FS')) AND (popsrc = 'D')",
                                   "LIMIT 100"))
head(comp_data)

#Merge year 2001 data on year 200 data using nested dbGetQuery
qstring = paste("SELECT a.*, b.at AS at_2001, b.revt AS revt_2001, b.ib AS ib_2001",
                "FROM",
                  "(SELECT gvkey, fyear, at, revt, ib",
                  " FROM compa.funda",
                  " WHERE (fyear = 2000) AND (consol = 'C') AND (datafmt = 'STD') AND (indfmt IN('INDL','FS')) AND (popsrc = 'D') ) AS a",
                "INNER JOIN",
                  "(SELECT gvkey, fyear, at, revt, ib",
                  " FROM compa.funda",
                  " WHERE (fyear = 2001) AND (consol = 'C') AND (datafmt = 'STD') AND (indfmt IN('INDL','FS')) AND (popsrc = 'D') ) AS b",
                "ON (a.gvkey = b.gvkey)")
comp_merge_data = dbGetQuery(wrds, qstring)
head(comp_merge_data)

```

Everything else is just standard SQL. Recall that analytic window functions (e.g., ROW_NUMBER() OVER(x PARTITION BY y)) are supported in PostgreSQL.

### Querying the database with dplyr

dplyr offers a tidyverse interface for working with WRDS. It allows you to manipulate database tables using the same grammar as you would for data.frames and tibbles. 

To use dplyr to query WRDS, begin by making a reference to a table of interest in the database:

```{r connection2}

#Reference annual Compustat table
comp_tbl = tbl(wrds, "funda") #alternatively with schema reference replace funda with in_schema("compa", "funda")

```

The above object, comp_tbl, points to the funda table in the database. This allows us to work with comp_tbl like it is a normal dataframe:

```{r tbl_dataframe}

#Table metadata
colnames(comp_tbl)[1:10]

#Apply conventional Compustat filters and limits to the year 2000
comp_tbl = comp_tbl %>%
   filter( fyear == 2000, consol == 'C', datafmt == 'STD', indfmt %in% c('INDL','FS'), popsrc == 'D' )

#Limit analysis to just a subset of variables
comp_tbl = comp_tbl %>% 
   dplyr::select( gvkey, fyear, at, revt, ib )

#Look at the data
comp_tbl

```

Notice that dplyr performs "lazy execution" when you query and manipulate tables. That is, instead of bringing the entire comp_tbl data.frame to your desktop, it will only show you a subset of the rows at a time. 

To bring down the entire filtered table, we apply the collect() verb to the object:

```{r tbl_collect}

#Collect into a tibble or a data.frame()
comp_tbl = comp_tbl %>%
   collect() # %>% sample_n(100, replace = FALSE)

#Examine data and metadata
head(comp_tbl)
dim(comp_tbl)

```

dplyr is great for working around the inability to create new tables on the WRDS cloud. For example, we can merge on outcomes from Compustat in the year 2001 to the year 2000 using the following code:

```{r dplyrjoin}

#Create fiscal year 2000 table
comp_2000 = tbl(wrds, "funda") %>% 
   filter( fyear == 2000, consol == 'C', datafmt == 'STD', indfmt %in% c('INDL','FS'), popsrc == 'D' ) %>%
   select( gvkey, at, revt )

#Create fiscal year 2001 table
comp_2001 = tbl(wrds, "funda") %>% 
   filter( fyear == 2001, consol == 'C', datafmt == 'STD', indfmt %in% c('INDL','FS'), popsrc == 'D' ) %>%
   select( gvkey, at, revt ) %>%
   rename( at_2001 = at, revt_2001 = revt )
      
#Inner join the 2000 and 2001 table
comp_joined = comp_2000 %>% 
   inner_join( y = comp_2001, by = c("gvkey"="gvkey"))

#Download and collect - dplyr only queries the database at this point!
comp_joined = comp_joined %>%
   collect() # can also cast as %>% data.frame()

#Summarise the outcomes
head(comp_joined, 10)
dim(comp_joined)

```

Notice that the above code allows us to avoid messy nesting of our queries within SQL. 

To see the actual query that dplyr executed, you can use the command show_query:

```{r showquery}

#Re-create the comp_joined query
comp_query = comp_2000 %>% 
   inner_join( y = comp_2001, by = c("gvkey"="gvkey") )

#Show the query
show_query(comp_query)

```

Here are a few tricks I have picked up using dplyr-based queries in R:

1. You can pass native PostgreSQL commands through the dplyr synthax. For example, PostgreSQL allows you to extract the year from a date variable using the command date_part('year', date_variable). You can pass this command through mutate: mutate( year_variable = date_part('year', date_variable) ). This is useful because normally the lubridate() package is used for date manipulation in R and it cannot be passed to SQL through dplyr. 

# Using R on the WRDS cloud

You can also access data using the Unix-style WRDS cloud. 

The WRDS cloud is a high-performance computing cluster. It is available to all WRDS users. It is accessed via SSH.

The WRDS cloud supports the R programming language. There are several advantage to using the WRDS cloud for R users, including:

1. Access to larger computing resources. WRDS sets you up with an instance of R on a node with 256 GB of RAM and 24 cores. Theoretically, all of this computing power is available to you if there are no other users sharing the node. Note, however, that there is no distributed computing across nodes. All data is shared across nodes in the cluster. 

1. More direct access to the data. There will be less "distance" between your R queries and the PostgreSQL database. In addition, local copies of all WRDS tables are stored on the cluster in the SAS bdat format (directory: `/wrds`). For large datasets, this can offer dramatic improvements.

1. Up to 500 GB of temporary storage space in your institution's scratch directory. 

The primary drawback of the WRDS cloud is that you cannot (directly) interact using RStudio. Instead, you must use an interactive R session from the Unix command line on the cluster or submit a batch job to execute an entire R script. 

A more complete description of the WRDS cloud is available at: <https://wrds-www.wharton.upenn.edu/pages/support/the-wrds-cloud/introduction-wrds-cloud/>.

## Connecting to the WRDS cloud using SSH

We can connect to the WRDS cluster with SSH using a SSH client program, such as MobaXTerm or Putty. 

The host name for the WRDS cloud is: `wrds-cloud.wharton.upenn.edu` (port 22). You can connect with your WRDS username and password. 

WRDS will connect you to an edge node in the cluster (identified by `wrds-cloud-loginX`). On the edge node there will be a copy of your home directory. 

Your home directory is located at: `/home/[institution]/[username]`. For example, my directory is `/home/wustl/dsovich`. Of course, you can always check your current directory entering `pwd` in the Unix command line, and you can switch directories using `cd`. 

If you absolutely insist upon using RStudio at all times, you can also SSH into the WRDS cloud with the following code:

```{r ssh_studio, warning = FALSE, message = FALSE}

#Load the ssh package
library(ssh)

```

```{r, eval=FALSE}

#Connect to the WRDS cloud (default port is 22)
session = ssh_connect(paste0(Sys.getenv("WRDS_NAME"), "@wrds-cloud.wharton.upenn.edu"))

#Example submit: look at your current working directory
wrds_directory = ssh_exec_wait(session, command = "pwd")

```

For more information on executing commands and scripts from RStudio with the SSH package, see the following link: <https://cran.r-project.org/web/packages/ssh/vignettes/intro.html>. 

## Directories and data storage in the WRDS cloud

Below are several important directories in the WRDS cloud:

**/home/[institution]/[username]**: This is your home directory. You are given 10 GB of permanent storage space in this folder. You should store the following items in this folder:

1. Program scripts, including your .Rprofile and .Renvironment

2. Data that you upload to the WRDS cloud, including external macroeconomic data, text data, etc.

3. Custom R packages and functions. 

4. Final output from programs needing to be exported to your local PC.  

To determine disk usage in your home directory, type in the Unix command `quota` from the edge node.  

**/scratch/[institution]**: This is the temporary directory. Your institution is given 500 GB of temporary storage space in this folder. Files that have not been *touched* in a week are deleted from this folder by a bot. You should only store temporary datasets in this folder. 

**/wrds**: This is the flat-file directory for WRDS data. You can access all the WRDS tables in SAS bdat form directly from this directory. 

**/usr/local/sas/grid**: This is the R and Python package directory (.libPaths()). You can look at all the packages installed by the WRDS team from this directory. You cannot edit this directory. 

To bring data on-and-off the WRDS cloud, simply connect to the WRDS cloud through your favorite SFTP program. 

For example, you can use Filezilla to connect to the WRDS cloud through the host `wrds-cloud.wharton.upenn.edu` (port 22.) A more detailed discussion is provided at the following link: <https://wrds-www.wharton.upenn.edu/pages/support/the-wrds-cloud/managing-data/>. 

## Interactive R jobs

You can start a command line instance of R on the compute nodes on the WRDS cloud. You cannot, unfortunately, start an instance of RStudio on the WRDS cloud. 

To do this, type the Unix command `qrsh` into the terminal. This will set you up with a session on one of the 256 GB RAM 24 core compute nodes. You can tell whether you are on a compute node by your address: `wrds-sasX-h`.

You can start an instance of R by typing in the following command into the terminal:

```{r, engine = 'bash', eval = FALSE}
R --no-save --no-restore
```

Note this is different than the usual `R.exe` that we input into a command line terminal on Linux or Windows environments. 

Once your R session is initialized, you can interact with it like you normally would. To quit the session, type in `q()`. You can then return to the edge node from the compute node by typing in `logout` in the terminal. 

Note that compute nodes do not have connections to the internet. Therefore, your R instances cannot use any APIs. They can only pull files from your directories or submit queries on the  WRDS cloud environment.  

## Batch jobs

You can also submit your R programs for execution in a batched manner.

Normally, this is simply done by inputting the command `Rscript [program_name].R` where `[program_name].R` is an R script stored in your home directory. 

However, submitting jobs on the WRDS cloud is different.

Batch jobs cannot be submitted directly to the command line. They must instead be submitted by a wrapper script which instructs the Grid Engine on how to call the R program.

For example, you can use Notepad++ to create the below wrapper script myProgram.sh to call the myProgram.r program (example taken from WRDS):

```{r, engine = 'bash', eval = FALSE}
#!/bin/bash #Sets the shell of the wrapper script to bash.
#$ -cwd #referring to current working directory can also cd to a different directory
R CMD BATCH --no-save --no-restore myProgram.r #note: can reference a different directory here
```

The batch job for myProgram.R can then be submitted using the `qsub` command: `qsub myProgram.sh`. 

Submitting the job frees up your command line (a nice feature). You can view the status of your currently-running R jobs with the `qstat` command (if any are running). 

You can look at a log of your printed output if you use the `sink()` function in your code. 

## Installing R Packages

The WRDS cloud already has 10,000+ packages installed. To see the list of packages, either input the R command `installed.packages()` into an R session or examine the list of packages in the directory `/usr/local/sas/grid`. 

If you need a package that is not supported by WRDS, begin by creating a new folder in your home directory titled `/lib/R`. This can be done using Filezilla or the terminal code `mkdir ~/lib/R`. 

Add this new directory to your `.libPaths()` in your .Rprofile. The best way to do this is by setting `.libPaths( c( .libPaths(), "~/lib/R") )`. 

Once this directory is created, you can install new packages using two different methods:

1. Download the package Tar ball from CRAN or Github. Open an R instance. Input the following command: `install.packages("~/[package_name].tar.gz", lib="~/lib/R")`. 

2. From the edge node, `cd` to `~/lib/R`. Download the package at its CRAN hyperlink using the following code: `wget http://cran.r-project.org/src/contrib/[package_name].tar.gz`. Then install the Tar ball as in the step above. 

For more details, see: <https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-r/installing-r-packages/>. 

#Some notes on WRDS databases

WRDS provides excellent documentation for their databases.

Some relevant links include:

1. The differences between Compustat and CRSP (e.g., universe of coverage) and how to link across datasets: <https://wrds-www.wharton.upenn.edu/pages/support/applications/linking-databases/linking-crsp-and-compustat/>. See also: <http://www.kaikaichen.com/?p=138>. 

2. dafa

#Summarizing the TRACE database

Link to the file documentation here:

This summary is based off the February 2019 build.


